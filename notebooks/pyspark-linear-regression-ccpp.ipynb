{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://blog.epigno.systems/2018/02/18/machine-learning-with-pyspark-linear-regression/\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/00294/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"spark\"\n",
    "os.environ[\"SPARK_MAJOR_VERSION\"] = \"2\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/user/maria_dev/datasets/CCPP/data.csv\"\n",
    "sc = pyspark.SparkContext(appName=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AT: double, V: double, AP: double, RH: double, PE: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sqlContext.read.options(header='true', inferschema='true', delimiter=',').csv(file_name)\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|                AT|                 V|                AP|                RH|                PE|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|              9568|              9568|              9568|              9568|              9568|\n",
      "|   mean| 19.65123118729102| 54.30580372073601|1013.2590781772603| 73.30897784280926| 454.3650094063554|\n",
      "| stddev|7.4524732296110825|12.707892998326784| 5.938783705811581|14.600268756728964|17.066994999803402|\n",
      "|    min|              1.81|             25.36|            992.89|             25.56|            420.26|\n",
      "|    max|             37.11|             81.56|            1033.3|            100.16|            495.76|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- AT: double (nullable = true)\n",
      " |-- V: double (nullable = true)\n",
      " |-- AP: double (nullable = true)\n",
      " |-- RH: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\"AT\", \"V\", \"AP\", \"RH\"]\n",
    "lr_data = data.select(col(\"PE\").alias(\"label\"), *features)\n",
    "lr_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = lr_data.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "lr = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, lr]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+-----+--------------------+--------------------+------------------+\n",
      "| label|   AT|    V|     AP|   RH|   unscaled_features|            features|        prediction|\n",
      "+------+-----+-----+-------+-----+--------------------+--------------------+------------------+\n",
      "|425.12|31.74|72.58|1007.26|59.58|[31.74,72.58,1007...|[4.25925684300654...| 428.0324809437086|\n",
      "|425.14|29.67|71.98|1005.16|67.75|[29.67,71.98,1005...|[3.98147922281046...|430.82367997457794|\n",
      "|425.21|32.19|69.13|1000.45|48.22|[32.19,69.13,1000...|[4.31964328217960...| 429.2244676866211|\n",
      "|425.28|31.91|67.83|1008.76|53.22|[31.91,67.83,1008...|[4.28206949780525...| 429.9086812268348|\n",
      "| 425.3| 30.9| 69.4|1003.53|66.69|[30.9,69.4,1003.5...|[4.14653548988349...|429.06578322910303|\n",
      "|425.34|31.73|74.67|1016.38|44.51|[31.73,74.67,1016...|[4.25791492213603...| 430.5394185266408|\n",
      "| 425.5| 29.0|69.13|1001.22|52.96|[29.0,69.13,1001....|[3.89157052448613...|434.80667035218136|\n",
      "|425.55|29.27|69.89|1015.21|67.16|[29.27,69.89,1015...|[3.92780238798996...| 432.9169241169294|\n",
      "|425.61|32.56|68.94|1007.12|58.18|[32.56,68.94,1007...|[4.36929435438856...|427.48881959873387|\n",
      "|425.71|30.26|72.58| 1007.0|65.51|[30.26,72.58,1007...|[4.06065255417070...| 430.0026887409816|\n",
      "|425.72|32.63|69.89|1013.85|41.66|[32.63,69.89,1013...|[4.37868780048215...| 430.1591586852596|\n",
      "|425.74|31.49|69.13|1009.33| 53.9|[31.49,69.13,1009...|[4.22570882124373...|430.36197791898354|\n",
      "|425.75|32.67|67.83|1007.81|42.16|[32.67,67.83,1007...|[4.38405548396420...| 430.0544540093248|\n",
      "|425.82|31.16|64.96|1001.71|44.85|[31.16,64.96,1001...|[4.18142543251682...| 432.8394266737504|\n",
      "|425.91| 31.4|66.54|1003.55|57.73|[31.4,66.54,1003....|[4.21363153340912...|  430.142742855106|\n",
      "|425.91| 32.4|68.14|1004.44|34.53|[32.4,68.14,1004....|[4.34782362046036...| 431.4442791993192|\n",
      "|425.98|31.42|69.13|1009.34|57.16|[31.42,69.13,1009...|[4.21631537515014...| 429.9974366206282|\n",
      "| 426.1|30.85|68.94|1007.31|68.99|[30.85,68.94,1007...|[4.13982588553093...| 429.1903083999308|\n",
      "|426.13|30.03| 67.9|1006.22|52.61|[30.03,67.9,1006....|[4.02978837414891...| 433.4908901242156|\n",
      "|426.14|25.88|69.13|1002.44|85.67|[25.88,69.13,1002...|[3.47289121288624...|435.97204795490404|\n",
      "+------+-----+-----+-------+-----+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.557\n",
      "MSE: 20.768\n",
      "MAE: 3.587\n",
      "r2: 0.929\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(prediction)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(prediction, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(prediction, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(prediction, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lr_data))\n",
    "print(type(training))\n",
    "print(type(test))\n",
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
